<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://taras-sereda.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://taras-sereda.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-09T01:33:33+00:00</updated><id>https://taras-sereda.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">NVIDIA GPUs fan control from terminal</title><link href="https://taras-sereda.github.io/blog/2024/nvidia-gpus-fan-control-from-terminal/" rel="alternate" type="text/html" title="NVIDIA GPUs fan control from terminal"/><published>2024-11-08T00:00:00+00:00</published><updated>2024-11-08T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2024/nvidia-gpus-fan-control-from-terminal</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2024/nvidia-gpus-fan-control-from-terminal/"><![CDATA[<p>It’s a long standing issue that NVIDIA doesn’t allow GPUs fan control via command line. NVIDIA only allows to control fans of consumer GPUs via GUI tool. That might be painful and inconvenient on headless Deep Learning rigs.</p> <p>A <a href="https://forums.developer.nvidia.com/t/how-to-set-fanspeed-in-linux-from-terminal/72705">relevant thread</a> stays open on NVIDIA devs forum for at least last 5 years. Official documentation is sparse on this matter, and third-party resources like <a href="https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks">tips&amp;trick on archlinux wiki</a> are way more informative.</p> <p>Bellow I describe an effective way of GPU fan control, tested on Ubuntu 22.04.</p> <ol> <li>Allow manual fan control by setting corresponding coolbits value in X11 config. <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo nvidia-xconfig -a --cool-bits=4
</code></pre></div> </div> </li> </ol> <p><code class="language-plaintext highlighter-rouge">-a</code> generates <code class="language-plaintext highlighter-rouge">xorgs.conf</code> for all GPUs available on your machine <code class="language-plaintext highlighter-rouge">--cool-bits=4</code> enables manual GPU fun speed configuration. You can read more about meaning of various coolbits values <a href="https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks#Overclocking_and_cooling">here</a></p> <p>Open <code class="language-plaintext highlighter-rouge">/etc/X11/xorgs.conf</code> and make sure that number of <code class="language-plaintext highlighter-rouge">Device</code> and <code class="language-plaintext highlighter-rouge">Screen</code> sections equal to number of GPUs on the server. Also make sure <code class="language-plaintext highlighter-rouge">Screen</code> sections reflects supplied <code class="language-plaintext highlighter-rouge">cool-bits</code> value.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Section "Device"
    Identifier     "Device0"
    Driver         "nvidia"
    VendorName     "NVIDIA Corporation"
    BoardName      "NVIDIA GeForce RTX 4090"
    BusID          "PCI:193:0:0"
EndSection

Section "Screen"
    Identifier     "Screen0"
    Device         "Device0"
    Monitor        "Monitor0"
    DefaultDepth    24
    Option         "AllowEmptyInitialConfiguration" "True"
    Option         "Coolbits" "4"
    SubSection     "Display"
        Depth       24
    EndSubSection
EndSection

...
</code></pre></div></div> <ol> <li>Now with manual fan speed configuration enabled it’s sufficient to call <code class="language-plaintext highlighter-rouge">nvidia-settings</code> emulating run of xserver. Script bellow accepts single argument that specifies fan speed for all GPUs. Calling <code class="language-plaintext highlighter-rouge">sudo gpufancontrol.sh 80</code> will set fans at 80% speed.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nb">set</span> <span class="nt">-eo</span> pipefail

<span class="nv">FAN_SPEED</span><span class="o">=</span><span class="nv">$1</span>

<span class="nv">DISPLAY</span><span class="o">=</span>:1
startx <span class="nt">--</span> <span class="k">${</span><span class="nv">DISPLAY</span><span class="k">}</span> &amp;
<span class="nb">sleep </span>1
nvidia-settings <span class="nt">-c</span> <span class="k">${</span><span class="nv">DISPLAY</span><span class="k">}</span> <span class="nt">-a</span> <span class="nv">GPUFanControlState</span><span class="o">=</span>1 <span class="nt">-a</span> <span class="nv">GPUTargetFanSpeed</span><span class="o">=</span><span class="k">${</span><span class="nv">FAN_SPEED</span><span class="k">}</span>
killall Xorg
<span class="nb">exit </span>1
</code></pre></div></div> <p>You can download script from <a href="https://gist.github.com/taras-sereda/15d1afb127a332721e008950574b972d#file-gpufancontrol-sh">gist</a>. That’s it, now you have an easy way of fan speed control from the command line. In my experience automatic GPUs NVIDIA tends to keep fans at lower speed, perhaps trying to optimise for noise level. But my servers sit in a basement and I care more about hardware being not overheated. Moreover I use cheap gaming PCIe raisers that tend to work unstable in higher temperatures. While writing this I’m re running a training that previously was failing due to PCIe errors. Peak observed temperatures were ~65C on average, fans were spinning at 40% speed. Now when I set fans at 95% speed, temperatures reduced to 45C. A stunning GPU temperature reduction by 20C, so far training is stable!</p>]]></content><author><name></name></author><category term="hardware,"/><category term="deep"/><category term="learning"/><summary type="html"><![CDATA[It’s a long standing issue that NVIDIA doesn’t allow GPUs fan control via command line. NVIDIA only allows to control fans of consumer GPUs via GUI tool. That might be painful and inconvenient on headless Deep Learning rigs.]]></summary></entry><entry><title type="html">Lecture Notes UCU-TTS. Lecture 2. Vocoders and Modern TTS</title><link href="https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-2-vocoders-and-modern-tts/" rel="alternate" type="text/html" title="Lecture Notes UCU-TTS. Lecture 2. Vocoders and Modern TTS"/><published>2024-04-25T00:00:00+00:00</published><updated>2024-04-25T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-2-vocoders-and-modern-tts</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-2-vocoders-and-modern-tts/"><![CDATA[<h2 id="introduction-to-vocoders">Introduction to vocoders</h2> <p>Most modern TTS systems, are in fact text-to-spectrogram systems. And once we obtained a spectrogram, more precisely a power spectrogram, we need a way to map it to the corresponding waveform. So Vocoders are spectrum to waveform models, and their primary task is to reconstruct the missing phase information, and we can think of them as inverse-spectrogram models.</p> \[\begin{align*} V: \mathbb{R}^{\text{F} \times \text{T}} \rightarrow \mathbb{R}^{\text{S}} \end{align*}\] <p>In case our TTS models outputs are not STFT-spectrograms but MEL-spectrograms, then task of vocoder is getting harder than merely phase reconstruction. But still we can think of this class of models as those that a given an arbitrary <em>incomplete</em> <strong>spectrum representation</strong> of the signal and are tasked to produce it’s corresponding <strong>time-domain representation</strong>.</p> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Group%201-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Group%201-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Group%201-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Group%201.png" title="image"/> </picture> </figure> </div> </div> <blockquote> <p>[!Question] If you are given a complex valued STFT spectrogram \(\text{Spectro} \in \mathbb{C}^{\text{F} \times \text{T}}\), how would you obtain a corresponding time-domain waveform \(W \in \mathbb{R}^{\text{S}}\)?</p> </blockquote> <p>Griffin-Lim classic and simple approach for phase reconstruction was used in tacotron<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> paper. The proposed method starts with randomly initialized phase and runs a series of \(STFT-iSTFT\) operations with <em>fixed</em> input spectrum magnitude and <em>iteratively refined</em> phase.</p> <blockquote> <p>[!reminder] In polar notation complex numbers are represented as \(z=re^{i\varphi}\)</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># adapted from: https://github.com/rbarghou/pygriffinlim

import librosa
import numpy as np
from IPython.display import display, Audio

def griffin_lim(spectrogram, n_iter=32, approximated_signal=None, stft_kwargs={}):
    _M = spectrogram
    for k in range(n_iter):
        if approximated_signal is None:
            _P = np.random.randn(*_M.shape)
        else:
            _D = librosa.stft(approximated_signal, **stft_kwargs)
            _P = np.angle(_D)

        _D = _M * np.exp(1j * _P)
        approximated_signal = librosa.istft(_D, **stft_kwargs)
    return approximated_signal

stft_kwargs = {'n_fft' : 2048}

audio_path = './data/LJ001-0051.wav'
waveform, sr = librosa.load(audio_path)

spectro = librosa.stft(waveform, **stft_kwargs)
spectro_mag = np.abs(spectro)

waveform_hat = griffin_lim(spectro_mag, stft_kwargs=stft_kwargs)

display(Audio(audio_path))
display(Audio(waveform_hat, rate=sr))

</code></pre></div></div> <p>That said first valuable results in neural TTS modelling were obtained with very simple spectrogram to waveform inversion technique! Further adoption of Griffin-Lim was tampered with it’s applicability only to STFT spectrograms, when our aim is to process other spectral representations such as MEL-spectrograms, gamma-tones or any other signals represented in their spectral form.</p> <h2 id="neural-vocoders">Neural Vocoders</h2> <p>Vocoders have access to the whole spectrogram and intuitively they only need local information (few spectral frames) to do the spectrogram inversion. This allows us to run them in parallel, leveraging transposed-convolutions for temporal upsampling. HiFi-GAN<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> is fast and modern vocoder employing GANs to produce raw waveforms. This model effectively addresses local and global consistency via multi-period and multi-scale discriminators. Where multi-period discriminators sample points every <code class="language-plaintext highlighter-rouge">[2, 3, 5, 7, 11]</code> steps and is responsible for global consistency. In its turn multi-scale discriminator operates on raw audio, ×2 average-pooled audio, and ×4 average-pooled audio and is responsible for local consistency of the patches of various temporal resolutions.</p> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Group%202-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Group%202-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Group%202-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Group%202.png" title="image"/> </picture> </figure> </div> </div> <h2 id="objective-metrics-for-tts-systems">Objective Metrics for TTS systems</h2> <p>In TTS modelling we care about both semantics and acoustics of the utterance. More specifically by semantics we mean preserved content and meaning of the phrase, and this aspect can be measured by computing <strong>WER</strong> on input text and transcripts obtained from generated utterances via separate ASR model. Low WER values signifies that semantic information was preserved by the model. By acoustic aspects of generated speech we mean proper prosody, preserved voice of the target speaker, room acoustics.</p> <p>To measure acoustics properties of the generated speech we employ separate <strong>speaker recognition</strong> models like WavLM-TDNN<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">3</a></sup> and compute cosine similarity between real sample of the speaker and generated speech conditioned on voice-vector of the same speaker. Speaker similarity scores quantify ability of the TTS model in preserving of acoustical properties of the speech.</p> <p>We can move further and measure <strong>similarity of prosody</strong>, by measuring FID<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">4</a></sup> between samples from real and synthesized distributions, the lower FID is the closer is prosody of synthesized sampled to real data. Another classic measure is Mean Cepstral Distortion(<strong>MCD</strong>).It’s aimed to measure distortions of acoustic information, this metric is widely used because it highly correlates with human perception. All metrics described above are neatly assembled in the repository for MQTTS<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">5</a></sup></p> <h2 id="discrete-speech-representations">Discrete Speech Representations</h2> <p>To bridge the gap and apply language modelling techniques to TTS field we need to obtain discrete representations for speech.</p> <h3 id="hubert">HuBERT</h3> <p>When it comes to semantics, we can leverage <strong>HuBERT</strong><sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">6</a></sup> a self-supervised model trained with masked language modelling objective. This model allows us to extract discrete tokens from input raw waveform. HuBERT is trained to reconstruct both masked and unmasked tokens with resulting loss function defined as: \(L = \alpha L_{m} + (1 − \alpha)L_{u}\) , where \(\alpha \in [0..1]\) is a hyper parameter that controls impact of each of the loss components. With \(\alpha = 1\) shift focus towards learning language modelling capabilities, when \(\alpha=0\) loss is computed only on unmasked tokens, that is similar to acoustic modelling.</p> <p>HuBERT has no reconstruction objective, we can’t be sure that acoustic information is preserved in enough amount for applicability of these type of tokens for acoustic modelling. So what we obtain from the model is phone-like features, or data-driven neural-IPA-like phonemes! And this can be validated by training an ASR model with CTC-loss on top of the learned representations.</p> <h3 id="encodec">EnCodec</h3> <p>So we need a way to obtain acoustic tokens. <strong>EnCodec</strong><sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">7</a></sup> is a SOTA model that is trained to compress and tokenize speech signals into discrete units via Residual Vector Quantization (RVQ). EnCodec is trained to reconstruct inputs signals both in time-domain and spectral representations, additionally multi-scale discriminators are added to further improve quality of reconstruction.</p> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Group%204-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Group%204-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Group%204-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Group%204.png" title="image"/> </picture> </figure> </div> </div> <h3 id="speech-tokenizer">Speech Tokenizer</h3> <p>Now that we have both semantic tokens, that capture sequence modelling and text meaning, together with acoustic tokens that hold acoustic information we want to unify them and build a joint <em>semantic-acoustic representation</em> that can be leveraged for TTS modelling! That’s is precisely the idea of SpeechTokenizer<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">8</a></sup> , where HuBERT tokens are used for guided semantic residual quantization that allows to disentangle speech and store semantic information in the first quantizer $Q_{0}$ . Where all subsequent $Q-1$ qauntizers are responsible for preserving of acoustic information.</p> <p>Whole pipeline is trained with both distillation and reconstruction objectives making SpeechTokenizer tokens suitable for downstream tasks as TTS. Bellow is definition of the distillation objective, that enforces similarity in each of the dimensions across all the time steps of the first quantizer features. This formulation intuitively facilitates long-range distillation constancy across all the timestep of the given utterance.</p> \[\begin{align*} L_{\text{distill}} = -\frac{1}{D} \sum_{d=1}^{D} \log \sigma(\cos(\text{AQ}^{(:,d)}, \text{S}^{(:,d)})) \end{align*}\] <blockquote> <p>[!Question] Speech Tokenizer disentangles content from acoustics, can you think of a way how would you further disentangle prosody or room characteristics?</p> </blockquote> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Group%205-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Group%205-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Group%205-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Group%205.png" title="image"/> </picture> </figure> </div> </div> <h2 id="pheme">Pheme</h2> <p>Language models that are explicitly optimised with next token prediction objective have a nice property of sequence completion for provided input prefix. This property is relevant for dialog modelling where our dataset is represented as pairs of utterances and their corresponding texts. \(D = \{ [(x^{(i)}_{0}, y^{(i)}_{0}), (x^{(i)}_{1}, y^{(i)}_{1})] \}_{i=1}^{N}\) , where \(x\) - is an audio clip, and \(y\) - is an uttered text. We say that pair \((x_0, y_0)\) is a prompt phrase where pairs \((x_1, y_1)\) is a corresponding continuation of the prompt. We want to design such a model that will maintain <em>tone</em> and <em>prosody</em> that are most relevant for the provided prompt prefix. Next token prediction models excel in this task! Building upon ideas of fast and parallel decoding introduced in SoundStorm<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">9</a></sup> as well as leveraging disentangled representations obtained from SpeechTokenizer our research group at Poly-AI designed Pheme<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">10</a></sup> parallel faster than real-time neural conversational TTS.</p> <p>Similar to SoundStorm we split TTS modelling pipeline into two components. First is an autoregressive next-token prediction model that maps input text(phonemes) to semantic features with help of T5<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">11</a></sup> architecture. We trained several models of various sizes: 100M, 150M and 300M parameters. Intuitively text-to-semantics model is responsible for <em>learning alignment</em> between input phonemes and corresponding semantic features. At inference time we provide both prefixes to encoder and decoder of the prompt phrase as well as text of the second phrase as encoder’s input. The task of the model now is to predict such a semantic features sequence that follows input text and is coherent with the semantic prompt.</p> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020240411004906-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020240411004906-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020240411004906-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Pasted%20image%2020240411004906.png" title="image"/> </picture> </figure> </div> </div> <p>Second model is designed to predict acoustic features given semantic inputs and target speaker embeddings. We employ conformer architecture and leverage MaskGIT<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">12</a></sup> scheme that allows us to predict masked unit at the given quantization level leveraging information from surrounding context of unmasked units. This approach allows to do parallel decoding in time dimension and only requires to do sequential decoding in quantizers dimension. We obtain faster then realtime inference because number of quantizers \(Q \in [8, 12, 16]\) is significantly smaller then number of time steps.</p> <p>Our goal is to train a zero-shot conversational TTS that speaks with various voices that are extracted with help of third-party voice-vector model. We extract voice features from the reference sample, that serves as a global conditioning of the semantic-to-acoustic decoder. Our design choice of splitting semantic and acoustic modelling is motivated by the observation that for acoustic modelling we don’t even need texts, so this allows us to use vast amount of audio only training data. When training of text-to-semantics model still requires pairs of text and their corresponding utterances for extract of semantic features. An ongoing research is concerned with training Pheme in multilingual context. Training language specific text-to-semantics model and reusing semantic-to-acoustics model training only for English shows promising results and suggests that multilingual TTS in Pheme framework can be achieved by combining language specific semantics models with a single shared acoustic model trained on multiple languages.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/31036.wav" controls=""/> </figure> </div> <p>We open sourced Pheme, and I also started to work on adapting Pheme to Ukrainian language, this is an ongoing effort. I encourage you to join this initiative and bring your contributions to open-source Ukrainian TTS models.</p> <p>Another bit of self-advertisement, I started a new project aimed to unify audio-preparation utils for corpus creation. I write it in Rust, since I want to maximally leverage available machine resources when it comes to high volumes audio data. At the moment of writing audio-utils<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">13</a></sup> allows you to compute total duration of all audio files in the folder or manifest in no time! even if your folder contains hundreds of thousands of small audio clips. Just recently I integrated symphonia CLI audio-player analogous to sox’s <code class="language-plaintext highlighter-rouge">play</code> . Short term plans are to add support of <code class="language-plaintext highlighter-rouge">onnxruntime</code> that will allow to use pretrained models for various corpus-preparation tasks, as audio trimming, ASR inference, voice-vectors extraction from literally any onnx compatible model.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><a href="https://arxiv.org/pdf/1703.10135v2.pdf">Tactron</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2010.05646">HiFi-GAN</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2110.13900">WavLM</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7" role="doc-endnote"> <p><a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">FID</a> <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8" role="doc-endnote"> <p><a href="https://github.com/b04901014/MQTTS/tree/main/measures">MQTTS</a> <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2106.07447">HuBERT</a> <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2210.13438">EnCodec</a> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2308.16692">SpeechTokenizer</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2305.09636">SoundStorm</a> <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2401.02839">Pheme</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:12" role="doc-endnote"> <p><a href="https://arxiv.org/abs/1910.10683">T5</a> <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2202.04200">MaskGIT</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:14" role="doc-endnote"> <p><a href="https://github.com/taras-sereda/audio-utils">audio-utils</a> <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="teaching"/><category term="ML"/><summary type="html"><![CDATA[Introduction to vocoders Most modern TTS systems, are in fact text-to-spectrogram systems. And once we obtained a spectrogram, more precisely a power spectrogram, we need a way to map it to the corresponding waveform. So Vocoders are spectrum to waveform models, and their primary task is to reconstruct the missing phase information, and we can think of them as inverse-spectrogram models.]]></summary></entry><entry><title type="html">Lecture Notes UCU-TTS. Lecture 1. Introduction</title><link href="https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-1-introduction/" rel="alternate" type="text/html" title="Lecture Notes UCU-TTS. Lecture 1. Introduction"/><published>2024-04-22T00:00:00+00:00</published><updated>2024-04-22T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-1-introduction</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-1-introduction/"><![CDATA[<h2 id="overview">Overview</h2> <p>As of April 2024 we’ve experienced a remarkable progress in ML for speech synthesis. Modern TTS models take a lot of inspiration from NLP field in particular by employing transformers as in VALL-E<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">1</a></sup>, using masked language modelling for learning self-supervised representations as in HuBERT<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">2</a></sup> or WavLM<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">3</a></sup> (importantly WavLM jointly learns masked speech prediction and denoising, that makes this model robust to various acoustic conditions). Another important line of work uses learnable residual vector-quantization for building of neural codecs that efficiently compress speech, music and audio signal in general. SoundStream<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">4</a></sup> is one of such models that beats state of the art codecs on very low bitrates, 3kbps.</p> <h2 id="training-any-tts-model-requires">Training any TTS model requires:</h2> <ul> <li>Segmented and aligned <strong>dataset</strong>, pairs of texts and corresponding audio clips. Covering multiple speakers, speaking styles and acoustic conditions. \(D = \{ (x^{(i)}, y^{(i)}) \}_{i=1}^{N}\) Modern TTS models are trained on hundreds even thousands of hours. For research purposes it’s sufficient to train model on tens of hours, to get reasonable model performance. One of the classic datasets for English TTS is LJSpeech<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">5</a></sup> , this is a single speaker dataset with total length of approximately 24 hours.</li> <li><strong>Model architectures</strong> that leverage inductive biases and observations in the domain of speech, audio, seq-to-seq modelling. Speech contains a lot of redundancy that makes compression possible. Ideas that allow to bridge the gap between highly compressed text representation and audio waveform will be helpful in building robust and resource efficient TTS models. As showcased in the most recent models FACodec<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">6</a></sup> and speech trident survey<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">7</a></sup></li> <li><strong>Compute</strong>. TTS models take significant time to converge, modern models are trained on multi-node multi-gpu clusters and take weeks or even months to converge. But again as with datasets it’s sufficient to have access even to a single-node multi-gpu instance with modern GPUs like RTX 4090 or RTX3090 to effectively experiment in the field of TTS. I’m training my models on 6 RTX3090 GPU rig, and this configuration is sufficient for majority of Speech processing model I’m working on. You are lucky if you can train models on DGX instances with V100s, take most out of them!</li> </ul> <h2 id="data-preparation">Data Preparation</h2> <p>TTS model the process of reading text out loud, consequently to avoid ambiguities input texts have to be properly preprocessed. Preprocessing pipeline are language dependent and on a high level view consist of:</p> <ul> <li> <p><em>text normalization</em> includes short form expansion, conversion of numbers to words. All this can be achieved in deterministic way with help of WFST(weighted finite state transducer) models. They are implemented for multiple languages in NeMo text processing toolkit<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">8</a></sup> You can find a well detailed introduction to WFSTs in Jurafsky’s book Speech and Language Processing<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup></p> <p>Example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  St.Patrick's day is a good reason to treat yourself with that $30 bottle of wine! 
  -&gt; 
  Saint Patricks day is a good reason to treat yourself with that thirty dollar bottle of wine!
</code></pre></div> </div> </li> <li> <p><em>phonemization</em> common next step in text preprocessing is to convert written text to phonemes, all because phonemes are more closely mapped to underlying acoustics. Today’s universal approach is to do IPA phonemization with help of espeak-ng. Phonemizer<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">10</a></sup> is good python library for this task, it supports multiple backends and allows to run phonemization for over 100+ of languages. This step is particularly important for languages with complicated spelling as English, but might be of less necessity for Spanish that is more strait forward in spelling. Overall this makes task of TTS easier, and makes possible convergence with less amount of data and compute. Example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Saint Patricks day is a good reason to treat yourself with that thirty dollar bottle of wine!
  -&gt; 
  sˈeɪnt pˈatɹɪks dˈeɪ ɪz ɐ ɡˈʊd ɹˈiːzən tə tɹˈiːt jɔːsˈɛlf wɪð ðat θˈɜːti dˈɒlə bˈɒtəl ɒv wˈaɪn
</code></pre></div> </div> </li> </ul> <h2 id="alignments">Alignments</h2> <p>Another important aspect of data preparation is verification of the correspondence between written and spoken text. Often times TTS datasets are automatically segmented from web crawled long-form recordings such as audio books and youtube videos. It’s common to see datapoints with some of the missing or redundant words in text. Excessive regions without speech at the beginning or end of the utterance also might significantly deteriorate performance of the TTS model.</p> <blockquote> <p>[!Question] Can you think of other scenarios where alignments might be useful and for what applications?</p> </blockquote> <p>It’s a common practice to inspect the dataset and filter out problematic datapoints. Team at NVIDIA proposed a toolbox for construction and analysis of ASR datasets, that also can be used for TTS purposes<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">11</a></sup>. That approach allows to align unsegmented long-form recordings with the corresponding text, with consequential chunking and analysis of segmented data. CTC-based pretrained model is used to extract alignments as described in CTC-segmentation paper<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">12</a></sup>, poorly aligned regions are discarded. Final filtering is done by measuring WER and removing clips with low correspondence between text and audio.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/audio-sample.wav" controls=""/> </figure> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word                    score    start   end
перша               	0.04	    0	  240
проблема            	0.11	  280	  840
це                  	0.28	  880	 1000
ситуація            	0.06	 1020	 1440
коли                	0.07	 1480	 1760
ти                  	0.00	 1820	 2100
прокачуеш           	0.07	 2140	 2640
себе                	0.00	 2660	 2820
на                  	0.20	 2880	 2980
максиму             	0.00	 3000	 3480
</code></pre></div></div> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020240411122731-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020240411122731-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020240411122731-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Pasted%20image%2020240411122731.png" title="image"/> </picture> </figure> </div> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020240411122919-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020240411122919-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020240411122919-1400.webp"/> <img class="img-fluid rounded" src="/assets/img/Pasted%20image%2020240411122919.png" title="image"/> </picture> </figure> </div> </div> <h2 id="tacotron2">Tacotron2</h2> <p>Back in 2017 tacotron2<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">13</a></sup>was a SOTA model, achieving human level quality with <code class="language-plaintext highlighter-rouge">MOS=4.5</code>. Now tacotron is a classic example of sequence-to-sequence modelling in pre-transformers era. Bellow I’ll expand a bit more on what tacotron is, how the model is trained, and where are its merits and limitations. Tacotron is a text to spectrogam model that consists of 3 major building blocks: characters(phoneme) encoder, attention block; auto-regressive decoder.</p> <p><strong>Encoder</strong> is represented as a stack of convolutional layers that take phone embeddings and output contextualized phones, that effect is attributed to convolutional nature of the encoder additional bi-directional LSTM allows to model long-term contexts and influences across all the characters of the sequence.</p> <p><strong>Attention</strong> block is the most important and hard to learn part of the model. Authors use location-sensitive attention that is described in attention based models for ASR<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">14</a></sup> Intuitively attention is represented as an <em>alignment matrix</em> between text and spectrogram frames. Where length of text is \(N\) characters and length of the spectrogram is \(M\) frames.</p> \[\begin{align*} A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1M} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2M} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NM} \end{bmatrix} , A \in \mathbb{R}^{N \times M} \end{align*}\] <p>Attention selects relevant characters that are mapped to the corresponding spectrogram frame. We do this by accounting previous alignment \(a_{i-1}\) too, so that model attends to element of the sequence that in close proximity to previous position. This encourages model to monotonically move forwards.</p> <blockquote> <p>[!Question] Can you think of alternatives on how text tokens can be mapped to their corresponding acoustic representations?</p> </blockquote> <p><strong>Decoder</strong> We use teacher forcing (ground truth previous frame is used for conditioning) and attention context from the encoder to predict next spectrogram frame. During inference teach-forcing is not possible so previously predicted spectrogram frames are used for the conditioning. Autoregressive nature of decoding is slow and grows with the length of the spectrogram.</p> <p><strong>Loss function.</strong> The model is trained using frame-wise MSE loss. Remember teacher forcing mentioned above? We need it exactly for the purpose of being able to compute MSE, since otherwise model will be less constrained in terms of modelling durations for each of the phonemes in the utterance. Our goal is to match number of frames in ground truth and predicted spectrograms as well as to have all the frames exactly aligned.</p> \[\begin{align*} \mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} \| x^{(i)} - \hat{x}^{(i)} \|^2, x^{(i)} \in \mathbb{R}^{F} \end{align*}\] <blockquote> <p>[!Question] What metrics can you think of for measuring quality of a TTS system?</p> </blockquote> <h2 id="ecosystem">Ecosystem</h2> <p>These days when deep learning for speech is getting mature, multiple libraries are available for training TTS models, the most prominent of them are:</p> <ul> <li>NVIDIA’s NeMo<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">15</a></sup></li> <li>ESPnet<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">16</a></sup> from CMU</li> <li>s3prl<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">17</a></sup> - speech self supervised learning toolkit</li> </ul> <p>These toolkits are good for training production quality models, though for research purposes I suggest to focus on standalone easily hackable repos with implementations of your interest so that you can iterate quicker while testing new ideas or learning new concept. I suggest exploring NeMo tutorials<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">18</a></sup> on a jupyter notebooks<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">19</a></sup> covering various TTS models including classic examples and modern approaches.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:12" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2301.02111">VALL-E</a> <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:13" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2106.07447">HuBERT</a> <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:14" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2110.13900">WavLM</a> <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:15" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2107.03312">SoundStream</a> <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p><a href="https://keithito.com/LJ-Speech-Dataset/">LJSpeech</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2403.03100">FACodec</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6" role="doc-endnote"> <p><a href="https://github.com/ga642381/speech-trident">SpeechTrident</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7" role="doc-endnote"> <p><a href="https://github.com/NVIDIA/NeMo-text-processing">NeMo text processing</a> <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8" role="doc-endnote"> <p><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a> <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9" role="doc-endnote"> <p><a href="https://github.com/bootphon/phonemizer">phonemizer</a> <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2104.04896">construction and analysis of speech datasets</a> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2007.09127">CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition</a> <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1" role="doc-endnote"> <p><a href="https://arxiv.org/abs/1712.05884">Tacotron2</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><a href="https://arxiv.org/abs/1506.07503">Attention-Based Models for Speech Recognition</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:16" role="doc-endnote"> <p><a href="https://github.com/NVIDIA/NeMo">NeMo</a> <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://github.com/espnet/espnet">ESPnet</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:18" role="doc-endnote"> <p><a href="https://github.com/s3prl/s3prl">s3prl</a> <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:17" role="doc-endnote"> <p><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tts/intro.html">NeMo TTS tutorials</a> <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:19" role="doc-endnote"> <p><a href="https://github.com/NVIDIA/NeMo/tree/main/tutorials/tts">NeMo TTS jupyter notebooks</a> <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="teaching"/><category term="ML"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Explaining cross-entropy to teenagers</title><link href="https://taras-sereda.github.io/blog/2023/short-geust-lecture-in-school/" rel="alternate" type="text/html" title="Explaining cross-entropy to teenagers"/><published>2023-12-01T00:00:00+00:00</published><updated>2023-12-01T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2023/short-geust-lecture-in-school</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2023/short-geust-lecture-in-school/"><![CDATA[<p>In Ukraine, we have been in a state of war for almost two years now, and I’m becoming seriously concerned about the state of science in our country. My mom is a math school teacher, and she once suggested that I come and give a short introductory talk about how logarithmic functions are used in training AI. This way, kids can have a better understanding of how the knowledge they are acquiring is used in building modern machine learning algorithms.</p> <p>I realized that explaining cross-entropy loss would be a good starting point to capture the curiosity of young minds and demonstrate a specific use case for the new concept they are learning, namely logarithmic functions.</p> <p>I came across a thread about KL divergence on HN. The top comment caught my attention because it explained KL divergence from a bottom-up approach, introducing concepts of surprise, expected surprise, and expected surprise of the other person, and gradually delving into KL divergence. Since the focus of this write-up is cross-entropy, I’ll stop there.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/unfair-die-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/unfair-die-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/unfair-die-1400.webp"/> <img class="img-fluid rounded z-depth-1" src="/assets/img/unfair-die.jpeg" title="image"/> </picture> </figure> </div> </div> <div class="caption"> weird unfair die, created with chatGPT </div> <p>So, I took a similar methodology. First, I needed to introduce what probability is. Using a model of a rolling die, I started to ask questions about the probabilities of different events. Then we began to reason together on how to express the degree of surprisal, ensuring it can’t be negative and should increase as the probability of a particular event decreases. This led us to: \(-\log p(x), \quad 0 &lt; x \leq 1\).</p> <p>Later, we arrived at the conclusion that the expected surprise for a random variable with an even probability distribution is equal to the surprise of any individual event. However, it gets more interesting when the probability density function (pdf) is not even anymore. Eventually, we discussed an unfair die, or a weird die that, for some reason, has the value of 4 on multiple faces. Now, our task is to measure the expected surprise of another person whose belief about the unfair die \(q\) is different from the actual one \(p\). This led us to cross-entropy, which expresses the expected surprise of a person or neural network whose beliefs differ from the actual ones: \(H (p,q) = - \sum p(x) \log q(x)\)</p> <p>So, to summarize, cross-entropy is used to measure how well predictions match real data distribution. The lower the cross-entropy, the better the model of the data distribution we have at our hands.</p> <p>I created short lecture <a href="https://taras-sereda.github.io/assets/pdf/logarithms-in-ml.pdf">notes</a> for this introduction, it’s written in Ukrainian using amazing <a href="https://typst.app/">typst</a>.</p>]]></content><author><name></name></author><category term="ML,"/><category term="teaching"/><summary type="html"><![CDATA[In Ukraine, we have been in a state of war for almost two years now, and I’m becoming seriously concerned about the state of science in our country. My mom is a math school teacher, and she once suggested that I come and give a short introductory talk about how logarithmic functions are used in training AI. This way, kids can have a better understanding of how the knowledge they are acquiring is used in building modern machine learning algorithms.]]></summary></entry><entry><title type="html">Setting Up a minimal vimrc for Python Development</title><link href="https://taras-sereda.github.io/blog/2023/minimal-vimrc-for-python/" rel="alternate" type="text/html" title="Setting Up a minimal vimrc for Python Development"/><published>2023-10-22T00:00:00+00:00</published><updated>2023-10-22T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2023/minimal-vimrc-for-python</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2023/minimal-vimrc-for-python/"><![CDATA[<p>I have tried many code editors and vim remains my top choice for both writing code and taking notes. In this blog post I’ll share how I setup vim as Python Development Environment. By adding a handful of plugins and bringing some keyboard shortcuts of your choice you can get a pretty decent development experience even on headless machines. Notably vim consumes fewer resources than PyCharm and it will not spend ridiculous amount of time scanning and indexing your projects. Vim just works!</p> <ol> <li> <p>First important aspect is plugin manager, that serves as a package managers for you editor. I like to use <a href="https://github.com/junegunn/vim-plug">vim-plug</a> It allows you to explicitly define all plugins you need in a single vim-centric config file <code class="language-plaintext highlighter-rouge">.vimrc</code>. With vim-plug you need to add corresponding git url inside plug’s begin/end block or even use a shorthand of a form <code class="language-plaintext highlighter-rouge">&lt;git-user&gt;/&lt;repo-name&gt;</code>. Example: <em><del>https://github.com/</del>avidhalter/jedi-vim</em></p> <p>Please consult official manual on how to install vim-plug. Bellow is minimal <code class="language-plaintext highlighter-rouge">.vimrc</code> for python developement that uses single amazing plugin <a href="https://github.com/davidhalter/jedi-vim">jedi-vim</a>. With jedi-vim you’ll have features like go-to-definition, variables renaming, and code completion.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> " Enable syntax highlighting
 syntax enable
 " Display line numbers to the left
 set number

 set backspace=indent,eol,start

 call plug#begin("~/.vim/plugged")

 Plug 'davidhalter/jedi-vim'
 call plug#end()
</code></pre></div> </div> <blockquote> <p>Tip: Don’t forget to activate virtual environment in a tab where you are launching vim, jedi-vim needs to have access to installed packages, if it can’t find a package it will not be able to do code completions.</p> </blockquote> </li> <li> <p>Vim with jedi is good enough to start with, though my productivity improved significantly by bringing up several more plugins. Another important feature of modern IDEA is linting and fixing of sources. This can be achieved by using great <a href="https://github.com/dense-analysis/ale">ALE</a> plugin, that supports Language Server Protocol(LSP). I like to use <a href="https://github.com/Microsoft/pyright">pyright</a> for linting and <a href="https://github.com/psf/black">black</a>, <a href="https://github.com/PyCQA/isort">isort</a> as my fixers.</p> <ul> <li> <p>Employing fixers of your choice you wouldn’t need to care about signle or double quotes for strings, code formating and improts reoredings.</p> </li> <li> <p>By using linters you’ll get <code class="language-plaintext highlighter-rouge">Warnings</code> and <code class="language-plaintext highlighter-rouge">Errors</code> in comments to corresponding lines, so you’ll be able to fix your code before trying to execute it.</p> </li> </ul> <blockquote> <p>Tip: Don’t forget to <code class="language-plaintext highlighter-rouge">pip install isort black pyright</code></p> </blockquote> <p>If you’ll consult <code class="language-plaintext highlighter-rouge">ALEFixSuggest</code> you’ll see that it offers general purpose fixers that can be used for any files. Namely <code class="language-plaintext highlighter-rouge">trim_whitespace</code> and <code class="language-plaintext highlighter-rouge">remove_trailing_lines</code>. By adding these fixers call to <code class="language-plaintext highlighter-rouge">:ALEFix</code> will do a mundane job for you.</p> </li> <li> <p>Another important aspect of productive coding is to search files quickly. For this I use <a href="https://github.com/junegunn/fzf.vim">fzf.vim</a> that allows to browse your project root and open files in current buffer or next tab. For convenience I keep several key board shortcuts in my <code class="language-plaintext highlighter-rouge">.vimrc</code> to quickly search necessary files.</p> </li> <li> <p>I like to observe git changes to the current file I’m working on. And <a href="https://github.com/airblade/vim-gitgutter">vim-gitgutter</a> is a plugin I use to bring git related semantics to the editor, namely ability to see changes, deletions, additions, as well as features like staging/unstaging of code hunks and navigation between them.</p> </li> </ol> <p>Bellow is the more feature complete version of <code class="language-plaintext highlighter-rouge">.vimrc</code>. Also you can grab my ever-changing <code class="language-plaintext highlighter-rouge">.vimrc</code> configured for both rust and python development from <a href="https://github.com/taras-sereda/dotfiles/blob/main/.vimrc">here</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>" Enable syntax highlighting
syntax enable
" Display line numbers to the left
set number
filetype plugin indent on
set backspace=indent,eol,start

call plug#begin("~/.vim/plugged")

Plug 'dense-analysis/ale'
let g:ale_linters = {'python': ['pyright']}
let g:ale_fixers = {'*': ['trim_whitespace', 'remove_trailing_lines'], 'python': ['black', 'isort']}

Plug 'davidhalter/jedi-vim'
Plug 'ervandew/supertab'
Plug 'junegunn/fzf', { 'do': { -&gt; fzf#install() } }
Plug 'junegunn/fzf.vim'
Plug 'airblade/vim-gitgutter'

call plug#end()

nnoremap &lt;silent&gt; &lt;C-f&gt; :Files&lt;CR&gt;
</code></pre></div></div> <p>I hope this guide will help you in setting up your own development environment, so that you’ll have full control on features you have in the editor.</p> <p>With love from Kyiv, Ukraine.</p>]]></content><author><name></name></author><category term="vim,"/><category term="software-engineering"/><summary type="html"><![CDATA[I have tried many code editors and vim remains my top choice for both writing code and taking notes. In this blog post I’ll share how I setup vim as Python Development Environment. By adding a handful of plugins and bringing some keyboard shortcuts of your choice you can get a pretty decent development experience even on headless machines. Notably vim consumes fewer resources than PyCharm and it will not spend ridiculous amount of time scanning and indexing your projects. Vim just works!]]></summary></entry><entry><title type="html">Thoughts on how modern ASR can be improved</title><link href="https://taras-sereda.github.io/blog/2023/asr-ideas-whisper/" rel="alternate" type="text/html" title="Thoughts on how modern ASR can be improved"/><published>2023-02-04T00:00:00+00:00</published><updated>2023-02-04T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2023/asr-ideas-whisper</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2023/asr-ideas-whisper/"><![CDATA[<p>These days I’m experimenting with Whisper ASR. This model has a remarkable accuracy for many languages, and what’s surprising is that the Word error rate (WER) for Spanish is one of the lowest across all the languages model was trained.</p> <p>Whisper was trained on 680,000 hours of audio, predominantly English, with only 117 hours covering all other 96 languages. I’ll be speculative now, though the following makes sense. English and Spanish are relatively close to each other, so training on English should already result in a model which should be easily adaptable to Spanish and perhaps other languages. Another observation is that Spanish phonetically and orthographically is more straightforward than English. In Spanish, what you write corresponds precisely to what you hear.</p> <p>On MLS (multi-lingual Librispeech), it has already reached 4.2 % WER - this is remarkable; it’s on par with native Spanish speakers’ capabilities. Imagine the degree of synergy if a human expert were involved only for problematic words, where ASR results are of low confidence.</p> <p>For my exploration, I picked <a href="https://elhilo.audio/">El Hilo</a> podcast produced by Radio Ambulante. This fact is important - because “Radio Ambulante” is a proper noun, it’s not common to see words Radio and Ambulante next to each other, it will be difficult for the model to recognize it correctly.</p> <audio controls="" src="https://sphinx.acast.com/p/acast/s/el-hilo/e/63d34c0fdd7a730010e0f4f3/media.mp3"/> <p>Here is what I’ve got intially:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>whisper_stt.transcribe(wav_clip)
{
	'text': ' Bienvenidos a El Hilo, un podcast de Radio Mulante Studio.',
	'avg_logprob': -0.313,
	'compression_ratio': 0.906,
}
</code></pre></div></div> <p>The result is okay, though <strong>Ambulante</strong> was recognized as <strong>Mulante</strong>. Whisper supports prompting, a kind of conditional information that should navigate the model’s outputs. So I tried to use prompts as dictionaries.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>whisper_stt.transcribe(wav_clip, initial_prompt="Ambulante")
{
	'text': ' Bienvenidos a El Hilo, un podcast de Radio Ambulante Studios.',
	'avg_logprob': -0.263,
	'compression_ratio': 0.897,
}
</code></pre></div></div> <p>Notice that now <strong>Ambulante</strong> is transcribed correctly. Also, the probability of this transcript increased. Interestingly, adding <strong>El Hilo</strong> to the prompt helped to increase probability even more. So the model pays attention to prompts and also “enjoys” seeing something similar to its hypothesis. It still struggles to transcribe <strong><span style="color:red">E</span>studios</strong>, perhaps due to the model being trained in a multilingual setting with a huge imbalance towards English data.</p> <p>Lastly, I decided to mislead the model</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>whisper_stt.transcribe(wav_clip, initial_prompt="Radio Aminilantes")
{
	'text': ' Bienvenidos a El Hilo, un podcast de Radio Aminilantes Studios.',
	'avg_logprob': -0.301,
	'compression_ratio': 0.913,
}
</code></pre></div></div> <p>Well, now it follows custom spelling and transcribes words paying attention to prompts; now we have <strong>Radio Aminilantes</strong> exactly the way we prompted it to be.</p> <p>Overall this is pretty cool, and now I’m wondering how transcriptions could be improved for domain-specific data with a lot of terminologies: medicine, etc. I think this is not a part of the model’s design and is a nice side effect, though a powerful one. There is no explicit structure or guarantee that model will pay attention to the prompt. However this can be designed explicitly and incorporated in the decoding process.</p> <p>Another idea is to use prompting for creating pronunciation dictionaries because today’s high-quality TTS models fail to pronounce foreign words that are uncommon in English. Let me know if you’d like to work on this problem; we could join our efforts. Also, if you’ll find more interesting behavior modes of whisper - please share your findings!</p>]]></content><author><name></name></author><category term="deep_learning,"/><category term="ASR,"/><category term="Whisper"/><summary type="html"><![CDATA[These days I’m experimenting with Whisper ASR. This model has a remarkable accuracy for many languages, and what’s surprising is that the Word error rate (WER) for Spanish is one of the lowest across all the languages model was trained.]]></summary></entry><entry><title type="html">i3wm for multilinguals</title><link href="https://taras-sereda.github.io/blog/2022/i3-for-multilinguals/" rel="alternate" type="text/html" title="i3wm for multilinguals"/><published>2022-07-16T00:00:00+00:00</published><updated>2022-07-16T00:00:00+00:00</updated><id>https://taras-sereda.github.io/blog/2022/i3-for-multilinguals</id><content type="html" xml:base="https://taras-sereda.github.io/blog/2022/i3-for-multilinguals/"><![CDATA[<p>One day I’ve heard about <a href="https://i3wm.org/">i3 tiling window manager</a>. I thought, it looks like <code class="language-plaintext highlighter-rouge">tmux</code> applied to GUI applications. (it turned out to be even more powerful!) So I decided to install Linux with i3 window manager on my mac book pro, and use it as an alternative to macOS, which I like a lot.</p> <p><em>Disclaimer:</em> using i3 is an adventure. You need to configure many things. The learning curve is steep, though along the way you are building an understanding that you can do whatever you can imagine with Linux and i3 as a window manager.</p> <p>In this post I want to share my experience on how to add second input language. In my case I was interested in adding Ukrainian.</p> <ol> <li> <p><strong>Adding a new language.</strong></p> <p>i3 customization can be done via editing <code class="language-plaintext highlighter-rouge">~/.i3/config</code> . My native language is Ukrainian, so to add it as one more language for input the following line should be added to i3 config.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>exec --no-startup-id "setxkbmap -model pc105 -layout us,ua -option grp:alt_shift_toggle"
</code></pre></div> </div> <p>After rereading config, <code class="language-plaintext highlighter-rouge">alt+shift</code> will toggle between US and UA layouts. Basically that’s it! though there is more to say on this topic…</p> </li> <li> <p><strong>Handling unexpected issue.</strong></p> <p>Now when you have more than one language to choose from, another problem sooner or later will arise. :) By default, your computer will be locked in 10 min, if it’s in an idle state. This can be viewed and configured in the same <code class="language-plaintext highlighter-rouge">~/.i3/config</code>.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>exec --no-startup-id xautolock -time 10 -locker blurlock
</code></pre></div> </div> <p>When your screen is locked, unfortunately, there is no way to change the keyboard layout. It was a disappointment for me, luckily it can be fixed. As stated above <code class="language-plaintext highlighter-rouge">xautolock</code> will use <code class="language-plaintext highlighter-rouge">blurlock</code> as a locker.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>taras@archibald ~]<span class="nv">$ </span><span class="nb">cat</span> <span class="sb">`</span>which blurlock<span class="sb">`</span>
<span class="c">#!/bin/bash</span>
<span class="nb">set</span> <span class="nt">-eu</span>
   
<span class="nv">RESOLUTION</span><span class="o">=</span><span class="si">$(</span>xrandr <span class="nt">-q</span>|sed <span class="nt">-n</span> <span class="s1">'s/.*current[ ]\([0-9]*\) x \([0-9]*\),.*/\1x\2/p'</span><span class="si">)</span>
   
<span class="c"># lock the screen</span>
import <span class="nt">-silent</span> <span class="nt">-window</span> root jpeg:- | convert - <span class="nt">-scale</span> 20% <span class="nt">-blur</span> 0x2.5 <span class="nt">-resize</span> 500% RGB:- | <span class="se">\</span>
    i3lock <span class="nt">--raw</span> <span class="nv">$RESOLUTION</span>:rgb <span class="nt">-i</span> /dev/stdin <span class="nt">-e</span> <span class="nv">$@</span>
   
<span class="c"># sleep 1 adds a small delay to prevent possible race conditions with suspend</span>
<span class="nb">sleep </span>1
   
<span class="nb">exit </span>0
</code></pre></div> </div> <p>Since <code class="language-plaintext highlighter-rouge">blurlock</code> is essentially a bash script nothing prevents us from modifying it a little bit. I found a handy package <code class="language-plaintext highlighter-rouge">xkb-swith</code> that allows to set and query layouts. So before locking the screen, we should check current layout, and if it’s not US we forcefully set it to US. This will guarantee that we’ll be on the correct layout on a locked screen. That’s it!</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># set US layout if it's not US</span>
<span class="nv">current_layout</span><span class="o">=</span><span class="si">$(</span>xkb-switch<span class="si">)</span>
<span class="k">if</span> <span class="o">[[</span> <span class="nv">$current_layout</span> <span class="o">!=</span> <span class="s1">'us'</span> <span class="o">]]</span>
<span class="k">then
    </span>xkb-switch <span class="nt">-s</span> us
<span class="k">fi</span>
</code></pre></div> </div> </li> <li> <p><strong>Adding language indicator to i3bar.</strong></p> <p>By default, <code class="language-plaintext highlighter-rouge">i3bar</code> displays useful data about your system, like CPU load, amount of free space, battery charge level, etc. This data is supplied by <code class="language-plaintext highlighter-rouge">i3status</code> . But it doesn’t show a language indicator. So to added it to the <code class="language-plaintext highlighter-rouge">i3bar</code> I used same <code class="language-plaintext highlighter-rouge">xkb-swith</code> and wrote a custom wrapper over <code class="language-plaintext highlighter-rouge">i3status</code> as suggested in its man page.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>taras@archibald ~]<span class="nv">$ </span><span class="nb">cat</span> ~/scripts/my_i3status.sh 
<span class="c">#!/bin/sh</span>
<span class="c"># shell script to prepend i3status with more stuff</span>
   
i3status | <span class="k">while</span> :
<span class="k">do
    </span><span class="nb">read </span>line
    <span class="nv">current_layout</span><span class="o">=</span><span class="si">$(</span>xkb-switch<span class="si">)</span>
    <span class="nb">echo</span> <span class="s2">"</span><span class="nv">$current_layout</span><span class="s2"> | </span><span class="nv">$line</span><span class="s2">"</span> <span class="o">||</span> <span class="nb">exit </span>1
<span class="k">done</span>
</code></pre></div> </div> <p>Lastly it’s necessary to modify <code class="language-plaintext highlighter-rouge">~/.i3/config</code> to pipe output of <code class="language-plaintext highlighter-rouge">my_i3status.sh</code> to the <code class="language-plaintext highlighter-rouge">i3bar</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Start i3bar to display a workspace bar (plus the system information i3status if available)
bar {
	i3bar_command i3bar
	status_command ~/scripts/my_i3status.sh
	position bottom
	...
</code></pre></div> </div> <p>After re reading config you should have language indicator displayed on your bar!</p> <p><img src="/assets/img/1657470528.png" alt="i3statusbar"/></p> </li> </ol> <p>That’s all! Now switching languages doesn’t feel awkward. :)</p>]]></content><author><name></name></author><category term="linux"/><category term="productivity"/><category term="i3wm"/><summary type="html"><![CDATA[One day I’ve heard about i3 tiling window manager. I thought, it looks like tmux applied to GUI applications. (it turned out to be even more powerful!) So I decided to install Linux with i3 window manager on my mac book pro, and use it as an alternative to macOS, which I like a lot.]]></summary></entry></feed>