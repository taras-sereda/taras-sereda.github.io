<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Lecture Notes UCU-TTS. Lecture 1. Introduction | Taras Sereda</title> <meta name="author" content="Taras Sereda"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🇺🇦</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://taras-sereda.github.io/blog/2024/lecture-notes-ucu-tts-lecture-1-introduction/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-QYRRM4TV9Q"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QYRRM4TV9Q");</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://taras-sereda.github.io/"><span class="font-weight-bold">Taras</span> Sereda</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Resume-Taras-Sereda.pdf" target="_blank">resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Lecture Notes UCU-TTS. Lecture 1. Introduction</h1> <p class="post-meta">April 22, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/teaching"> <i class="fas fa-hashtag fa-sm"></i> teaching</a>   <a href="/blog/tag/ML"> <i class="fas fa-hashtag fa-sm"></i> ML</a>   </p> </header> <article class="post-content"> <h2 id="overview">Overview</h2> <p>As of April 2024 we’ve experienced a remarkable progress in ML for speech synthesis. Modern TTS models take a lot of inspiration from NLP field in particular by employing transformers as in VALL-E<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">1</a></sup>, using masked language modelling for learning self-supervised representations as in HuBERT<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">2</a></sup> or WavLM<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">3</a></sup> (importantly WavLM jointly learns masked speech prediction and denoising, that makes this model robust to various acoustic conditions). Another important line of work uses learnable residual vector-quantization for building of neural codecs that efficiently compress speech, music and audio signal in general. SoundStream<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">4</a></sup> is one of such models that beats state of the art codecs on very low bitrates, 3kbps.</p> <h2 id="training-any-tts-model-requires">Training any TTS model requires:</h2> <ul> <li>Segmented and aligned <strong>dataset</strong>, pairs of texts and corresponding audio clips. Covering multiple speakers, speaking styles and acoustic conditions. \(D = \{ (x^{(i)}, y^{(i)}) \}_{i=1}^{N}\) Modern TTS models are trained on hundreds even thousands of hours. For research purposes it’s sufficient to train model on tens of hours, to get reasonable model performance. One of the classic datasets for English TTS is LJSpeech<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">5</a></sup> , this is a single speaker dataset with total length of approximately 24 hours.</li> <li> <strong>Model architectures</strong> that leverage inductive biases and observations in the domain of speech, audio, seq-to-seq modelling. Speech contains a lot of redundancy that makes compression possible. Ideas that allow to bridge the gap between highly compressed text representation and audio waveform will be helpful in building robust and resource efficient TTS models. As showcased in the most recent models FACodec<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">6</a></sup> and speech trident survey<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">7</a></sup> </li> <li> <strong>Compute</strong>. TTS models take significant time to converge, modern models are trained on multi-node multi-gpu clusters and take weeks or even months to converge. But again as with datasets it’s sufficient to have access even to a single-node multi-gpu instance with modern GPUs like RTX 4090 or RTX3090 to effectively experiment in the field of TTS. I’m training my models on 6 RTX3090 GPU rig, and this configuration is sufficient for majority of Speech processing model I’m working on. You are lucky if you can train models on DGX instances with V100s, take most out of them!</li> </ul> <h2 id="data-preparation">Data Preparation</h2> <p>TTS model the process of reading text out loud, consequently to avoid ambiguities input texts have to be properly preprocessed. Preprocessing pipeline are language dependent and on a high level view consist of:</p> <ul> <li> <p><em>text normalization</em> includes short form expansion, conversion of numbers to words. All this can be achieved in deterministic way with help of WFST(weighted finite state transducer) models. They are implemented for multiple languages in NeMo text processing toolkit<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">8</a></sup> You can find a well detailed introduction to WFSTs in Jurafsky’s book Speech and Language Processing<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup></p> <p>Example:</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  St.Patrick's day is a good reason to treat yourself with that $30 bottle of wine! 
  -&gt; 
  Saint Patricks day is a good reason to treat yourself with that thirty dollar bottle of wine!
</code></pre></div> </div> </li> <li> <p><em>phonemization</em> common next step in text preprocessing is to convert written text to phonemes, all because phonemes are more closely mapped to underlying acoustics. Today’s universal approach is to do IPA phonemization with help of espeak-ng. Phonemizer<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">10</a></sup> is good python library for this task, it supports multiple backends and allows to run phonemization for over 100+ of languages. This step is particularly important for languages with complicated spelling as English, but might be of less necessity for Spanish that is more strait forward in spelling. Overall this makes task of TTS easier, and makes possible convergence with less amount of data and compute. Example:</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  Saint Patricks day is a good reason to treat yourself with that thirty dollar bottle of wine!
  -&gt; 
  sˈeɪnt pˈatɹɪks dˈeɪ ɪz ɐ ɡˈʊd ɹˈiːzən tə tɹˈiːt jɔːsˈɛlf wɪð ðat θˈɜːti dˈɒlə bˈɒtəl ɒv wˈaɪn
</code></pre></div> </div> </li> </ul> <h2 id="alignments">Alignments</h2> <p>Another important aspect of data preparation is verification of the correspondence between written and spoken text. Often times TTS datasets are automatically segmented from web crawled long-form recordings such as audio books and youtube videos. It’s common to see datapoints with some of the missing or redundant words in text. Excessive regions without speech at the beginning or end of the utterance also might significantly deteriorate performance of the TTS model.</p> <blockquote> <p>[!Question] Can you think of other scenarios where alignments might be useful and for what applications?</p> </blockquote> <p>It’s a common practice to inspect the dataset and filter out problematic datapoints. Team at NVIDIA proposed a toolbox for construction and analysis of ASR datasets, that also can be used for TTS purposes<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">11</a></sup>. That approach allows to align unsegmented long-form recordings with the corresponding text, with consequential chunking and analysis of segmented data. CTC-based pretrained model is used to extract alignments as described in CTC-segmentation paper<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">12</a></sup>, poorly aligned regions are discarded. Final filtering is done by measuring WER and removing clips with low correspondence between text and audio.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/audio-sample.wav" controls=""></audio> </figure> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word                    score    start   end
перша               	0.04	    0	  240
проблема            	0.11	  280	  840
це                  	0.28	  880	 1000
ситуація            	0.06	 1020	 1440
коли                	0.07	 1480	 1760
ти                  	0.00	 1820	 2100
прокачуеш           	0.07	 2140	 2640
себе                	0.00	 2660	 2820
на                  	0.20	 2880	 2980
максиму             	0.00	 3000	 3480
</code></pre></div></div> <div class="row justify-content-sm-center"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020240411122731-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020240411122731-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020240411122731-1400.webp"></source> <img class="img-fluid rounded" src="/assets/img/Pasted%20image%2020240411122731.png" title="image"> </picture> </figure> </div> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020240411122919-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020240411122919-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020240411122919-1400.webp"></source> <img class="img-fluid rounded" src="/assets/img/Pasted%20image%2020240411122919.png" title="image"> </picture> </figure> </div> </div> <h2 id="tacotron2">Tacotron2</h2> <p>Back in 2017 tacotron2<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">13</a></sup>was a SOTA model, achieving human level quality with <code class="language-plaintext highlighter-rouge">MOS=4.5</code>. Now tacotron is a classic example of sequence-to-sequence modelling in pre-transformers era. Bellow I’ll expand a bit more on what tacotron is, how the model is trained, and where are its merits and limitations. Tacotron is a text to spectrogam model that consists of 3 major building blocks: characters(phoneme) encoder, attention block; auto-regressive decoder.</p> <p><strong>Encoder</strong> is represented as a stack of convolutional layers that take phone embeddings and output contextualized phones, that effect is attributed to convolutional nature of the encoder additional bi-directional LSTM allows to model long-term contexts and influences across all the characters of the sequence.</p> <p><strong>Attention</strong> block is the most important and hard to learn part of the model. Authors use location-sensitive attention that is described in attention based models for ASR<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">14</a></sup> Intuitively attention is represented as an <em>alignment matrix</em> between text and spectrogram frames. Where length of text is \(N\) characters and length of the spectrogram is \(M\) frames.</p> \[\begin{align*} A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1M} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2M} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NM} \end{bmatrix} , A \in \mathbb{R}^{N \times M} \end{align*}\] <p>Attention selects relevant characters that are mapped to the corresponding spectrogram frame. We do this by accounting previous alignment \(a_{i-1}\) too, so that model attends to element of the sequence that in close proximity to previous position. This encourages model to monotonically move forwards.</p> <blockquote> <p>[!Question] Can you think of alternatives on how text tokens can be mapped to their corresponding acoustic representations?</p> </blockquote> <p><strong>Decoder</strong> We use teacher forcing (ground truth previous frame is used for conditioning) and attention context from the encoder to predict next spectrogram frame. During inference teach-forcing is not possible so previously predicted spectrogram frames are used for the conditioning. Autoregressive nature of decoding is slow and grows with the length of the spectrogram.</p> <p><strong>Loss function.</strong> The model is trained using frame-wise MSE loss. Remember teacher forcing mentioned above? We need it exactly for the purpose of being able to compute MSE, since otherwise model will be less constrained in terms of modelling durations for each of the phonemes in the utterance. Our goal is to match number of frames in ground truth and predicted spectrograms as well as to have all the frames exactly aligned.</p> \[\begin{align*} \mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} \| x^{(i)} - \hat{x}^{(i)} \|^2, x^{(i)} \in \mathbb{R}^{F} \end{align*}\] <blockquote> <p>[!Question] What metrics can you think of for measuring quality of a TTS system?</p> </blockquote> <h2 id="ecosystem">Ecosystem</h2> <p>These days when deep learning for speech is getting mature, multiple libraries are available for training TTS models, the most prominent of them are:</p> <ul> <li>NVIDIA’s NeMo<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">15</a></sup> </li> <li>ESPnet<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">16</a></sup> from CMU</li> <li>s3prl<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">17</a></sup> - speech self supervised learning toolkit</li> </ul> <p>These toolkits are good for training production quality models, though for research purposes I suggest to focus on standalone easily hackable repos with implementations of your interest so that you can iterate quicker while testing new ideas or learning new concept. I suggest exploring NeMo tutorials<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">18</a></sup> on a jupyter notebooks<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">19</a></sup> covering various TTS models including classic examples and modern approaches.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:12" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2301.02111" target="_blank" rel="noopener noreferrer">VALL-E</a> <a href="#fnref:12" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:13" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2106.07447" target="_blank" rel="noopener noreferrer">HuBERT</a> <a href="#fnref:13" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:14" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2110.13900" target="_blank" rel="noopener noreferrer">WavLM</a> <a href="#fnref:14" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:15" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream</a> <a href="#fnref:15" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:4" role="doc-endnote"> <p><a href="https://keithito.com/LJ-Speech-Dataset/" target="_blank" rel="noopener noreferrer">LJSpeech</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:5" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2403.03100" target="_blank" rel="noopener noreferrer">FACodec</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:6" role="doc-endnote"> <p><a href="https://github.com/ga642381/speech-trident" target="_blank" rel="noopener noreferrer">SpeechTrident</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:7" role="doc-endnote"> <p><a href="https://github.com/NVIDIA/NeMo-text-processing" target="_blank" rel="noopener noreferrer">NeMo text processing</a> <a href="#fnref:7" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:8" role="doc-endnote"> <p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener noreferrer">Speech and Language Processing</a> <a href="#fnref:8" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:9" role="doc-endnote"> <p><a href="https://github.com/bootphon/phonemizer" target="_blank" rel="noopener noreferrer">phonemizer</a> <a href="#fnref:9" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:10" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2104.04896" target="_blank" rel="noopener noreferrer">construction and analysis of speech datasets</a> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:11" role="doc-endnote"> <p><a href="https://arxiv.org/abs/2007.09127" target="_blank" rel="noopener noreferrer">CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition</a> <a href="#fnref:11" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:1" role="doc-endnote"> <p><a href="https://arxiv.org/abs/1712.05884" target="_blank" rel="noopener noreferrer">Tacotron2</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><a href="https://arxiv.org/abs/1506.07503" target="_blank" rel="noopener noreferrer">Attention-Based Models for Speech Recognition</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:16" role="doc-endnote"> <p><a href="https://github.com/NVIDIA/NeMo" target="_blank" rel="noopener noreferrer">NeMo</a> <a href="#fnref:16" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://github.com/espnet/espnet" target="_blank" rel="noopener noreferrer">ESPnet</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:18" role="doc-endnote"> <p><a href="https://github.com/s3prl/s3prl" target="_blank" rel="noopener noreferrer">s3prl</a> <a href="#fnref:18" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:17" role="doc-endnote"> <p><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tts/intro.html" target="_blank" rel="noopener noreferrer">NeMo TTS tutorials</a> <a href="#fnref:17" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:19" role="doc-endnote"> <p><a href="https://github.com/NVIDIA/NeMo/tree/main/tutorials/tts" target="_blank" rel="noopener noreferrer">NeMo TTS jupyter notebooks</a> <a href="#fnref:19" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Taras Sereda. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>